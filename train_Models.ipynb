{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path = os.path.abspath('')\n",
    "path = path + \"/engineered_data_250ms_window80_step8.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "diction = {'Dany':0, 'Felix':1, 'Julian':2, 'Mark':3,'Martin':4,'Michele':5,'Paul':6}\n",
    "\n",
    "for index,row in df['Participant'].items():\n",
    "    df.loc[index,'Participant'] = diction[row]\n",
    "\n",
    "List = [\"Window\", \"Participant\", \"Run\", \"Path\"]\n",
    "for name in df.columns:\n",
    "    if \"Acceleration\" in name and not \"Linear\" in name:\n",
    "        List.append(name)\n",
    "\n",
    "df.drop(df.columns.difference(List),axis=1,inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "df['Path'] = df['Path'].replace({'straight': 0, 'circle': 1})\n",
    "unique_values = df['Path'].unique()\n",
    "print(\"Unique values in the 'path' column:\", unique_values)\n",
    "sample_values = df['Path'].sample(n=10, random_state=1)  # random_state ensures reproducibility\n",
    "print(\"Sample of the 'path' column:\\n\", sample_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#create train/test set\n",
    "#should maybe do random runs in the future\n",
    "X_Train = df[df['Run'] < 4].copy()\n",
    "X_Test = df[df['Run'] == 4].copy()\n",
    "\n",
    "# Extracting targets\n",
    "Y_Train = X_Train.pop('Participant')\n",
    "Y_Test = X_Test.pop('Participant')\n",
    "\n",
    "# Verifying the final shapes and content of datasets\n",
    "print(\"X_Train shape:\", X_Train.shape)\n",
    "print(\"X_Test shape:\", X_Test.shape)\n",
    "print(\"Y_Train shape:\", Y_Train.shape)\n",
    "print(\"Y_Test shape:\", Y_Test.shape)\n",
    "\n",
    "#Display a few rows to confirm correct data setup\n",
    "print(X_Train['Path'].sample(3))\n",
    "print(X_Test['Path'].sample(3))\n",
    "\n",
    "# In order to solve the NaN issue, i try to locate the values\n",
    "print(\"NaN in X_Train:\", X_Train.isnull().sum().sum())\n",
    "print(\"NaN in Y_Train:\", Y_Train.isnull().sum().sum())\n",
    "print(\"NaN in X_Test:\", X_Test.isnull().sum().sum())\n",
    "\n",
    "# Find rows with NaN values in X_Train\n",
    "nan_rows_X_Train = X_Train[X_Train.isnull().any(axis=1)]\n",
    "print(\"Rows with NaN values in X_Train:\")\n",
    "print(nan_rows_X_Train)\n",
    "\n",
    "# Find rows with NaN values in X_Test\n",
    "nan_rows_X_Test = X_Test[X_Test.isnull().any(axis=1)]\n",
    "print(\"Rows with NaN values in X_Test:\")\n",
    "print(nan_rows_X_Test)\n",
    "\n",
    "# Print the number of unique rows that have NaN in X_Train and X_Test\n",
    "print(\"Number of unique rows with NaN in X_Train:\", nan_rows_X_Train.shape[0])\n",
    "print(\"Number of unique rows with NaN in X_Test:\", nan_rows_X_Test.shape[0])\n",
    "\n",
    "# In order to solve the NaN issue in the SVM I will drop the the NaN values\n",
    "# Remove rows with NaN values from X_Train and X_Test\n",
    "X_Train_cleaned = X_Train.dropna()\n",
    "X_Test_cleaned = X_Test.dropna()\n",
    "\n",
    "# Ensure the target variables are aligned\n",
    "Y_Train_cleaned = Y_Train[X_Train_cleaned.index]\n",
    "Y_Test_cleaned = Y_Test[X_Test_cleaned.index]\n",
    "\n",
    "# Ensure Y_Train_cleaned and Y_Test_cleaned is of integer type\n",
    "Y_Train_cleaned = Y_Train_cleaned.astype(int)\n",
    "Y_Test_cleaned = Y_Test_cleaned.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check for NaN values in Y_Train\n",
    "print(\"NaN in Y_Train:\", Y_Train_cleaned.isnull().sum().sum())\n",
    "print(\"NaN in Y_Train:\", Y_Test_cleaned.isnull().sum().sum())\n",
    "\n",
    "# Check the unique values in Y_Train\n",
    "print(\"Unique values in Y_Train:\", Y_Train_cleaned.unique())\n",
    "print(\"Unique values in Y_Train:\", Y_Test_cleaned.unique())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_Train_scaled = scaler.fit_transform(X_Train_cleaned)\n",
    "\n",
    "# Transform the test data\n",
    "X_Test_scaled = scaler.transform(X_Test_cleaned)\n",
    "\n",
    "# Using the scaled data for training and prediction\n",
    "# Define and train the RBF kernel SVM model\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1)\n",
    "rbf.fit(X_Train_scaled, Y_Train_cleaned)\n",
    "\n",
    "# Define and train the polynomial kernel SVM model with degree 20\n",
    "poly = svm.SVC(kernel='poly', degree=20, C=1)\n",
    "poly.fit(X_Train_scaled, Y_Train_cleaned)\n",
    "\n",
    "# Define and train the linear kernel SVM model\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(X_Train_scaled, Y_Train_cleaned)\n",
    "\n",
    "# Make predictions on the test set using all three models\n",
    "y_pred_rbf = rbf.predict(X_Test_scaled)\n",
    "y_pred_poly = poly.predict(X_Test_scaled)\n",
    "y_pred_linear = clf.predict(X_Test_scaled)\n",
    "\n",
    "print(\"RBF Kernel SVM Predictions:\", y_pred_rbf)\n",
    "print(\"Polynomial Kernel SVM Predictions:\", y_pred_poly)\n",
    "print(\"Linear Kernel SVM Predictions:\", y_pred_linear)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Print the predictions and accuracy from all three models\n",
    "print(\"Accuracy (RBF):\", metrics.accuracy_score(Y_Test_cleaned, y_pred_rbf))\n",
    "\n",
    "print(\"Accuracy (Polynomial):\", metrics.accuracy_score(Y_Test_cleaned, y_pred_poly))\n",
    "\n",
    "print(\"Accuracy (Linear):\", metrics.accuracy_score(Y_Test_cleaned, y_pred_linear))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(tree_method = 'hist',objective='multi:softprob')\n",
    "model.fit(X_Train,Y_Train)\n",
    "y_pred = model.predict(X_Test)\n",
    "#Y_Test = Y_Test.astype('int')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_Test , y_pred))\n",
    "\n",
    "cm = confusion_matrix(Y_Test,y_pred)\n",
    "sns.heatmap(cm,annot=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = permutation_importance(model, X_Test, Y_Test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: \"\n",
    "\n",
    "        f\"{elapsed_time:.3f} seconds\")\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "columns = columns.str.replace('median_','').str.replace('get_','').str.replace('_',' ')\n",
    "\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=columns[sorted_idx])\n",
    "\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot the permutation importance with correct naming\n",
    "\n",
    "feature_importance = result.importances_mean\n",
    "\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "\n",
    "plt.yticks(pos, columns[sorted_idx])\n",
    "\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.title('Permutation Importance (test set)')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the number of unique runs per individual\n",
    "df.groupby(['Participant','Path'])['Run'].nunique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Run'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# a function to implement cross validation given the number of runs we want to use for training (we have 4 in total)\n",
    "def cross_validation(df, seed, model, training_runs = 3):\n",
    "    runs = df['Run'].unique() # typically [1,2,3,4] in our case\n",
    "\n",
    "\n",
    "    if training_runs >= len(runs):\n",
    "        raise ValueError(\"The number of training runs should be less than the total number of runs\")\n",
    "\n",
    "    # all possible combinations of runs given the number of runs we want to use for training\n",
    "\n",
    "\n",
    "    training_combinations = list(combinations(runs, training_runs))\n",
    "\n",
    "    print(f\"Number of training combinations: {len(training_combinations)}\")\n",
    "\n",
    "    print(f\"The combinations: {training_combinations}\")\n",
    "\n",
    "    # we will store the accuracy for each combination of runs\n",
    "\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for training_set in training_combinations:\n",
    "\n",
    "\n",
    "        X_Train = df[df['Run'].isin(training_set)].copy()\n",
    "\n",
    "        X_Test = df[~df['Run'].isin(training_set)].copy()\n",
    "\n",
    "        Y_Train = X_Train['Participant'].copy()\n",
    "\n",
    "        Y_Test = X_Test['Participant'].copy()\n",
    "\n",
    "        X_Test.drop(['Participant'],axis=1,inplace=True)\n",
    "\n",
    "        X_Train.drop(['Participant'],axis=1,inplace=True)\n",
    "\n",
    "        model.fit(X_Train,Y_Train)\n",
    "\n",
    "        y_pred = model.predict(X_Test)\n",
    "\n",
    "        Y_Test = Y_Test.astype('int')\n",
    "\n",
    "        accuracies.append(metrics.accuracy_score(Y_Test , y_pred))\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# convert the path column to an int representation\n",
    "\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "df_copy.loc[df_copy['Path'] == 'circle', \"Path\"] = 1\n",
    "\n",
    "df_copy.loc[df_copy['Path'] == 'straight',\"Path\"] = 0\n",
    "\n",
    "df_copy['Path'] = df_copy['Path'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = XGBClassifier(tree_method = 'hist',objective='multi:softprob')\n",
    "\n",
    "\n",
    "accuracies = cross_validation(df_copy, 42, model, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accuracies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the mean accuracy and the standard deviation for different number of runs and with or without the path column\n",
    "# create a stripplot to show the distribution of the accuracies with seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_without_path = df.copy(deep=True)\n",
    "\n",
    "df_without_path.drop(['Path'],axis=1,inplace=True)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# we will use 2, 3, and 4 runs for training\n",
    "for training_runs in [1,2, 3]:\n",
    "    # Assuming cross_validation is a function that returns a list of accuracies\n",
    "    accuracies = cross_validation(df_copy, 42, model, training_runs)\n",
    "    for acc in accuracies:\n",
    "        results_list.append({'Training runs': training_runs, 'accuracy': acc, 'Path': 'yes'})\n",
    "\n",
    "    accuracies = cross_validation(df_without_path, 42, model, training_runs)\n",
    "    for acc in accuracies:\n",
    "        results_list.append({'Training runs': training_runs, 'accuracy': acc, 'Path': 'no'})\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "results = pd.DataFrame(results_list)\n",
    "# plot the results\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.stripplot(x='Training runs', y='accuracy', hue='Path', data=results, jitter=True, dodge=True)\n",
    "\n",
    "plt.title(\"Accuracy for different number of training runs and with or without the path column\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.stripplot(x='Number of runs used for training', y='accuracy', hue='Path used', data=results, jitter=True, dodge=True)\n",
    "\n",
    "plt.title(\"Accuracy for different number of training runs and with or without the path column\")\n",
    "\n",
    "plt.show() \n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
